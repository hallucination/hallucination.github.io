<!DOCTYPE HTML>
<html>
	<head>
		<title>Qi Feng | 冯 起</title>
		<link rel="icon" type="image/x-icon" href="/images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo"><a href="#">Qi Feng</a></h1>
					<p>Ph.D. in CG and CV /> 
					Assistant Professor<br />
					Waseda University, Morishima Lab</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">Profile</a></li>
						<li><a href="#two">Publications</a></li>
						<li><a href="#three">Projects</a></li>
						<li><a href="#four">Contact</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://github.com/HAL-lucination/" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/qi-feng-2680a6149/" class="icon brands fa-linkedin"><span class="label">Email</span></a></li>
						<li><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&authuser=1&user=17y69bAAAAAJ" class="icon solid fa-graduation-cap"><span class="label">Google Scholar</span></a></li>
						<li><a href="mailto: fengqi@ruri.waseda.jp" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						<br />
						<li><a href="#">English</a></li>
						<li><a href="#">日本語</a></li>
						<li><a href="#">Français</a></li><br />
						<li><a href="#">简体中文</a></li>
						<li><a href="#">繁體中文</a></li>
						
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="image main" data-position="center">
									<img src="images/banner.jpg" alt="" />
								</div>
								<div class="container">
									<header class="major">
										<h2>Qi Feng</h2>
										<h4><p> Obtained Ph.D. major in image processing this September. <br />
											Currently working as an assistant professor in Waseda University, Japan, Tokyo, <br />
											Graduate School of Advanced Science and Engineering, <br />
											Department of Pure and Applied Physics, <a href="https://morishima-lab.jp/">Morishima Lab</a>.</p></h4>
									</header>
									<div class="features">
										<article>
												<h3>Research Interest</h3>
												<h5><p>I specialize in using deep learning methods to solve computer graphics and computer vision problems.
													I am also interested in utilizing CG/CV methods to tackle real-world challenges in Virtual Reality (VR) and Augmented Reality (AR). </p></h5>
										</article>
										<article>
												<h3>Education</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>Sept. 2019 - Sept. 2022</h4>
													Doctor of Philosophy - Ph.D. <br />
													Waseda University </li>
													<li class="icon solid fa-graduation-cap"><h4>Sept. 2017 - Sept. 2019</h4>
													Master of Engineering - M.E. <br />
													Waseda University </li>
													<li class="icon solid fa-graduation-cap"><h4>Sept. 2013 - Sept. 2017</h4>
													Bachelor of Science - B.S. <br />
													Waseda University </li>
													<li class="icon solid fa-graduation-cap"><h4>Sept. 2010 - Sept. 2013</h4>
													Senior High School Graduate <br />
													High School Affiliated to Fudan University </li>
												</ul>
										</article>
										<article>
												<h3>Experience</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>Assistant Professor</h4>
													Apr. 2021 - Present<br />
													Waseda University </li>
													<li class="icon solid fa-graduation-cap"><h4>Research Intern</h4>
													Oct. 2019 - Mar. 2020 <br />
													Northumbria University </li>
													<li class="icon solid fa-graduation-cap"><h4>Research Intern</h4>
													Jul. 2019 - Sept. 2019 <br />
													National Institute of Advanced Industrial Science and Technology (AIST) </li>
													<li class="icon solid fa-graduation-cap"><h4>Research Intern</h4>
													Jul. 2015 - Sept. 2015 <br />
													Fudan University </li>
												</ul>
										</article>
										<article>
												<h3>Skills</h3>
												<h4>Language</h4>
												<p><b>Chinese</b> - Native &nbsp;&nbsp;&nbsp;&nbsp; <b>English</b> - Full professional proficiency (GRE score 325) <br />
												<b>Japanese</b> - Full professional proficiency (JLPT N1) &nbsp;&nbsp;&nbsp;&nbsp; <b>French</b> - Elementary proficiency (CEFR A2) </p>
												<h4>Programming Language</h4>
												<p>Advance: <b>Python</b>, HTML/CSS, SQL &nbsp;&nbsp;&nbsp;&nbsp; Familiar: C++, C#, Java, Javascript</p>
												<h4>Libraries/Frameworks/Platforms</h4>
												<p>PyTorch, Torchvision, Tensorflow, OpenCV &nbsp;&nbsp;&nbsp;&nbsp; Git, SharePoint &nbsp;&nbsp;&nbsp;&nbsp;  WordPress, Unity3D, Arduino</p>
												<h4>Deep Learning-related Topics</h4>
												<p>Object Classification, Semantic segmentation, Depth prediction, Scene reconstruction, Pose estimation, Style transfer, Data synthesis</p>
												<h4>Others</h4>
												<p>Microsoft Office 365 (Access, SharePoint), Adobe Creative Cloud (Lightroom Classic, Photoshop, Illustrator, After Effects, Premiere Pro, Audition, InDesign), Ableton, Vocaloid, Blender</p><br />
										</article>
									</div>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<div class="features">
										<article>
												<h3>Publications</h3>
												<h4>Journals</h4>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2020). <b>Resolving hand‐object occlusion for mixed reality with joint deep learning and model optimization.</b>
													Computer Animation and Virtual Worlds, 31(4-5), e1956.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b>, Journal of WSCG, 28(1-2), 79-88.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> The Visual Computer, 36(10), 2117-2128.</p>
												<p>Nozawa, N., Shum, H. P., <b>Feng, Q.</b>, Ho, E. S., & Morishima, S. (2021). <b>3D car shape reconstruction from a contour sketch using GAN and lazy learning.</b> The Visual Computer, 1-14.</p>
												<h4>Conferences</h4>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2022). <b>360 Depth Estimation in the Wild-The Depth360 Dataset and the SegFuse Network.</b> In 2022 IEEE conference on virtual reality and 3D user interfaces (VR). IEEE.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2021). <b>Bi-projection-based Foreground-aware Omnidirectional Depth Prediction.</b> Visual Computing Symposium 2021.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b> International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2020.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> Computer Graphics International 2020.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2018, November). <b>Resolving occlusion for 3D object manipulation with hands in mixed reality.</b> In Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology.</p>
												<p><b>Feng, Q.</b>, Nozawa, T., Shum, H. P., & Morishima, S. (2018, August). <b>Occlusion for 3D Object Manipulation with Hands in Augmented Reality.</b> In Proceedings of The 21st Meeting on Image Recognition and Understanding.</p>
										</article>
									</div>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>Projects</h3>
									<p>Open-souce projects released on Github.</p>
									<div class="features">
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/segfuse.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">360 Depth Estimation in the Wild</a></h4>
												<p>We present a method for generating large amounts of color/depth training data from abundant internet 360 videos,
													and propose a multitasking network to learn single-view depth estimation from it. Results show consistent and sharp predictions.</p>
											</div>
										</article>
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/foreground.jpg" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">Foreground-aware Dense Depth Estimation for 360 Images</a></h4>
												<p>We augment 360 depth datasets with realistic foregrounds and propose an auxiliary network
													to estimate depth/mask of the foreground, and achieved more accurate estimations.</p>
											</div>
										</article>
										<article>
											<a href="https://github.com/HAL-lucination/cyclegan-hand" class="image"><img src="images/hand.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://github.com/HAL-lucination/cyclegan-hand">Resolving Hand-Object Occlusion in Mixed Reality</a></h4>
												<p>We proposed an occlusion-aware RGB-D hand dataset using Cycle-GAN for pose tracking and segmentation under strong occlusions.
													We then solve MR occlusion in real time using optimization.</p>
											</div>
										</article>
									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>Contact Me</h3>
									<p>Tel: +81-3-5286-3510<br />
										Fax: +81-3-5286-3510<br />
										Address: <a href="https://goo.gl/maps/qCkFEBYzue7V8PMCA">55N406 3-4-1 Okubo, Shinjuku-ku, Tokyo, 169-0072</a><br />
										Email: fengqi[at]ruri.waseda.jp</p>
									<form action="http://formspree.io/fengqi@ruri.waseda.jp" method="POST">
										<div class="row gtr-uniform">
											<div class="col-6 col-12-xsmall"><input type="text" name="name" id="name" placeholder="Name" /></div>
											<div class="col-6 col-12-xsmall"><input type="email" name="email" id="email" placeholder="Email" /></div>
											<div class="col-12"><input type="text" name="subject" id="subject" placeholder="Subject" /></div>
											<div class="col-12"><textarea name="message" id="message" placeholder="Message" rows="6"></textarea></div>
											<div class="col-12">
												<ul class="actions">
													<li><input type="submit" class="primary" value="Send Message" /></li>
													<li><input type="reset" value="Reset Form" /></li>
												</ul>
											</div>
										</div>
									</form>
								</div>
							</section>
					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Qi Feng All rights reserved.</li><li>Last update: 2022 Feb. 10</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
