<!DOCTYPE HTML>
<html>
	<head>
		<title>Qi Feng | 馮 起</title>
		<link rel="icon" type="image/x-icon" href="/images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo"><a href="#">馮 起</a></h1>
					<p>圖像處理和機器視覺 Ph.D. <br /> 
					助理教授 <br />
					早稻田大學, 森島研究室</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">個人簡介</a></li>
						<li><a href="#two">論文成果</a></li>
						<li><a href="#three">研究主題</a></li>
						<li><a href="#four">聯繫方式</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://github.com/HAL-lucination/" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/qi-feng-2680a6149/" class="icon brands fa-linkedin"><span class="label">Email</span></a></li>
						<li><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&authuser=1&user=17y69bAAAAAJ" class="icon solid fa-graduation-cap"><span class="label">Google Scholar</span></a></li>
						<li><a href="mailto: fengqi@ruri.waseda.jp" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						<br />
						<li><a href="/index.html">English</a></li>
						<li><a href="/ja.html">日本語</a></li>
						<li><a href="/fr.html">Français</a></li><br />
						<li><a href="/zh-cn.html">简体中文</a></li>
						<li><a href="/zh-cht.html">繁體中文</a></li>
						
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="image main" data-position="center">
									<img src="images/banner.jpg" alt="" />
								</div>
								<div class="container">
									<header class="major">
										<h2>馮 起</h2>
										<h4><p> 我於今年九月獲得圖像處理專業的博士學位。<br />
											從2021年起在位於日本東京的早稻田大學 <br />
											先進理工學部擔任助理教授(Assistant Professor)職位。<br />
											我隸屬於<a href="https://morishima-lab.jp/">森島研究室</a>進行圖像處理和機器視覺相關的研究工作。</p></h4>
									</header>
									<div class="features">
										<article>
												<h3>研究方向</h3>
												<h5><p> 我的研究方向主要是通過基於深度學習的方法解決不同的計算機圖形和計算機視覺問題。
													除此之外，我也熱衷於活用CG/CV的方法來解決一些當前虛擬現實(VR)和增強現實(AR)應用中存在的一系列實際問題。</p></h5>
										</article>
										<article>
												<h3>教育經歷</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>2019年9月 - 2022年9月</h4>
													工程博士學位 - Ph.D. <br />
													早稻田大學 </li>
													<li class="icon solid fa-graduation-cap"><h4>2017年9月 - 2019年9月</h4>
													工程碩士學位 - M.E. <br />
													早稻田大學 </li>
													<li class="icon solid fa-graduation-cap"><h4>2013年9月 - 2017年9月</h4>
													工學學士學位 - B.E. <br />
													早稻田大學 </li>
													<li class="icon solid fa-graduation-cap"><h4>2010年9月 - 2013年9月</h4>
													高中畢業 <br />
													復旦大學附屬中學 </li>
												</ul>
										</article>
										<article>
												<h3>工作經歷</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>助理教授</h4>
													2021年4月 - 至今<br />
													早稻田大學 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究交換</h4>
													2019年10月 - 2020年3月 <br />
													諾桑比亞大學 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究實習</h4>
													2019年7月 - 2019年9月 <br />
													日本產業技術綜合研究所 (AIST) </li>
													<li class="icon solid fa-graduation-cap"><h4>研究實習</h4>
													2015年7月 - 2015年9月 <br />
													復旦大學 </li>
												</ul>
										</article>
										<article>
												<h3>技能一覽</h3>
												<h4>語言</h4>
												<p><b>中文</b> - 母語水平 &nbsp;&nbsp;&nbsp;&nbsp; <b>英語</b> - 熟練掌握 (GRE得分325) <br />
												<b>日語</b> - 熟練掌握 (JLPT N1) &nbsp;&nbsp;&nbsp;&nbsp; <b>法語</b> - 簡單對話 (CEFR A2) </p>
												<h4>編程語言</h4>
												<p>熟練使用: <b>Python</b>, HTML/CSS, SQL &nbsp;&nbsp;&nbsp;&nbsp; 較為熟悉: C++, C#, Java, Javascript</p>
												<h4>庫/架構/平台</h4>
												<p>PyTorch, Torchvision, Tensorflow, OpenCV &nbsp;&nbsp;&nbsp;&nbsp; Git, SharePoint &nbsp;&nbsp;&nbsp;&nbsp;  WordPress, Unity3D, Arduino</p>
												<h4>深度學習相關技能</h4>
												<p>對象分類, 圖像語義分割, 深度預測, 場景重建, 動作預測, 畫風轉換, 合成訓練集</p>
												<h4>其他</h4>
												<p>Microsoft Office 365 (Access, SharePoint), Adobe Creative Cloud (Lightroom Classic, Photoshop, Illustrator, After Effects, Premiere Pro, Audition, InDesign), Ableton, Vocaloid, Blender</p><br />
										</article>
									</div>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<div class="features">
										<article>
												<h3>論文成果</h3>
												<h4>雜誌期刊</h4>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2020). <b>Resolving hand‐object occlusion for mixed reality with joint deep learning and model optimization.</b>
													Computer Animation and Virtual Worlds, 31(4-5), e1956.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b>, Journal of WSCG, 28(1-2), 79-88.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> The Visual Computer, 36(10), 2117-2128.</p>
												<p>Nozawa, N., Shum, H. P., <b>Feng, Q.</b>, Ho, E. S., & Morishima, S. (2021). <b>3D car shape reconstruction from a contour sketch using GAN and lazy learning.</b> The Visual Computer, 1-14.</p>
												<h4>國際會議</h4>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2022). <b>360 Depth Estimation in the Wild-The Depth360 Dataset and the SegFuse Network.</b> In 2022 IEEE conference on virtual reality and 3D user interfaces (VR). IEEE.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2021). <b>Bi-projection-based Foreground-aware Omnidirectional Depth Prediction.</b> Visual Computing Symposium 2021.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b> International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2020.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> Computer Graphics International 2020.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2018, November). <b>Resolving occlusion for 3D object manipulation with hands in mixed reality.</b> In Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology.</p>
												<p><b>Feng, Q.</b>, Nozawa, T., Shum, H. P., & Morishima, S. (2018, August). <b>Occlusion for 3D Object Manipulation with Hands in Augmented Reality.</b> In Proceedings of The 21st Meeting on Image Recognition and Understanding.</p>
										</article>
									</div>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>研究主題</h3>
									<p>發布於Github上的開源項目: </p>
									<div class="features">
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/segfuse.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">360 Depth Estimation in the Wild</a></h4>
												<p>我們首先提出了一種從豐富的互聯網360度全景視頻中生成大量匹配的顏色/深度訓練數據的方法。
													其次我們提出了一個多任務網絡來學習360度圖像的單眼深度預測。實驗結果表面預測結果高效且準確。</p>
											</div>
										</article>
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/foreground.jpg" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">Foreground-aware Dense Depth Estimation for 360 Images</a></h4>
												<p>我們首先通過圖像處理的方法來獲取了不同樣式的前景顏色/深度的訓練數據。用新穎的方法將其合成至現有的360度圖像的訓練集後，
													我們提出了一個多任務輔助網絡和損失函數來利用生成的數據，成功克服了現有方法對前景對象預測結果較差的問題。</p>
											</div>
										</article>
										<article>
											<a href="https://github.com/HAL-lucination/cyclegan-hand" class="image"><img src="images/hand.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://github.com/HAL-lucination/cyclegan-hand">Resolving Hand-Object Occlusion in Mixed Reality</a></h4>
												<p>為了解決混合現實(MR)中常見的由於場景結構未知而導致的手與物體互動時的遮蔽問題，我們首先通過CycleGAN生成了大規模
													真實且準確的顏色/深度/姿勢訓練集，然後提出了一個基於姿勢和語義分割的多任務網絡的實時系統。使用者實驗和量化分析都獲得了很好的結果。</p>
											</div>
										</article>
									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>聯繫方式</h3>
									<p>電話: +81-3-5286-3510<br />
										傳真: +81-3-5286-3510<br />
										地址: <a href="https://goo.gl/maps/qCkFEBYzue7V8PMCA">55N406 3-4-1 Okubo, Shinjuku-ku, Tokyo, 169-0072</a><br />
										郵件: fengqi[at]ruri.waseda.jp</p>
									<form action="http://formspree.io/fengqi@ruri.waseda.jp" method="POST">
										<div class="row gtr-uniform">
											<div class="col-6 col-12-xsmall"><input type="text" name="name" id="name" placeholder="Name" /></div>
											<div class="col-6 col-12-xsmall"><input type="email" name="email" id="email" placeholder="Email" /></div>
											<div class="col-12"><input type="text" name="subject" id="subject" placeholder="Subject" /></div>
											<div class="col-12"><textarea name="message" id="message" placeholder="Message" rows="6"></textarea></div>
											<div class="col-12">
												<ul class="actions">
													<li><input type="submit" class="primary" value="Send Message" /></li>
													<li><input type="reset" value="Reset Form" /></li>
												</ul>
											</div>
										</div>
									</form>
								</div>
							</section>
					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Qi Feng All rights reserved.</li><li>Last update: 2022 Oct. 10</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
