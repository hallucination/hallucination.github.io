<!DOCTYPE HTML>
<html>
	<head>
		<title>Qi Feng | 冯 起</title>
		<link rel="icon" type="image/x-icon" href="/images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo"><a href="#">冯 起</a></h1>
					<p>图像处理和机器视觉 Ph.D. <br /> 
					助理教授 <br />
					早稻田大学, 森岛研究室</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">个人简介</a></li>
						<li><a href="#two">论文成果</a></li>
						<li><a href="#three">研究主题</a></li>
						<li><a href="#four">联系方式</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://github.com/HAL-lucination/" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/qi-feng-2680a6149/" class="icon brands fa-linkedin"><span class="label">Email</span></a></li>
						<li><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&authuser=1&user=17y69bAAAAAJ" class="icon solid fa-graduation-cap"><span class="label">Google Scholar</span></a></li>
						<li><a href="mailto: fengqi@ruri.waseda.jp" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						<br />
						<li><a href="/index.html">English</a></li>
						<li><a href="#">日本語</a></li>
						<li><a href="#">Français</a></li><br />
						<li><a href="/zh_cn.html">简体中文</a></li>
						<li><a href="#">繁體中文</a></li>
						
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="image main" data-position="center">
									<img src="images/banner.jpg" alt="" />
								</div>
								<div class="container">
									<header class="major">
										<h2>Qi Feng</h2>
										<h4><p> 我于今年九月获得图像处理专业的博士学位。<br />
											从2021年起在位于日本东京的早稻田大学 <br />
											先进理工学部担任助理教授(Assistant Professor)职位。<br />
											我隶属于<a href="https://morishima-lab.jp/">森岛研究室</a>进行图像处理和机器视觉相关的研究工作。</p></h4>
									</header>
									<div class="features">
										<article>
												<h3>研究方向</h3>
												<h5><p> 我的研究方向主要是通过基于深度学习的方法解决不同的计算机图形和计算机视觉问题。
													除此之外，我也热衷于活用CG/CV的方法来解决一些当前虚拟现实(VR)和增强现实(AR)应用中存在的一系列实际问题。</p></h5>
										</article>
										<article>
												<h3>Education</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>2019年9月 - 2022年9月</h4>
													工程博士学位 - Ph.D. <br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2017年9月 - 2019年9月</h4>
													工程硕士学位 - M.E. <br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2013年9月 - 2017年9月</h4>
													工学学士学位 - B.E. <br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2010年9月 - 2013年9月</h4>
													高中毕业 <br />
													复旦大学附属中学 </li>
												</ul>
										</article>
										<article>
												<h3>Experience</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>助理教授</h4>
													2021年4月 - 至今<br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究交换</h4>
													2019年10月 - 2020年3月 <br />
													诺桑比亚大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究实习</h4>
													2019年7月 - 2019年9月 <br />
													日本产业技术综合研究所 (AIST) </li>
													<li class="icon solid fa-graduation-cap"><h4>研究实习</h4>
													2015年7月 - 2015年9月 <br />
													复旦大学 </li>
												</ul>
										</article>
										<article>
												<h3>技能一览</h3>
												<h4>语言</h4>
												<p><b>中文</b> - 母语水平 &nbsp;&nbsp;&nbsp;&nbsp; <b>英语</b> - 熟练掌握 (GRE得分325) <br />
												<b>日语</b> - 熟练掌握 (JLPT N1) &nbsp;&nbsp;&nbsp;&nbsp; <b>法语</b> - 简单对话 (CEFR A2) </p>
												<h4>编程语言</h4>
												<p>熟练使用: <b>Python</b>, HTML/CSS, SQL &nbsp;&nbsp;&nbsp;&nbsp; 较为熟悉: C++, C#, Java, Javascript</p>
												<h4>库/架构/平台</h4>
												<p>PyTorch, Torchvision, Tensorflow, OpenCV &nbsp;&nbsp;&nbsp;&nbsp; Git, SharePoint &nbsp;&nbsp;&nbsp;&nbsp;  WordPress, Unity3D, Arduino</p>
												<h4>深度学习相关技能</h4>
												<p>对象分类, 图像语义分割, 深度预测, 场景重建, 动作预测, 画风转换, 合成训练集</p>
												<h4>其他</h4>
												<p>Microsoft Office 365 (Access, SharePoint), Adobe Creative Cloud (Lightroom Classic, Photoshop, Illustrator, After Effects, Premiere Pro, Audition, InDesign), Ableton, Vocaloid, Blender</p><br />
										</article>
									</div>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<div class="features">
										<article>
												<h3>论文成果</h3>
												<h4>杂志期刊</h4>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2020). <b>Resolving hand‐object occlusion for mixed reality with joint deep learning and model optimization.</b>
													Computer Animation and Virtual Worlds, 31(4-5), e1956.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b>, Journal of WSCG, 28(1-2), 79-88.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> The Visual Computer, 36(10), 2117-2128.</p>
												<p>Nozawa, N., Shum, H. P., <b>Feng, Q.</b>, Ho, E. S., & Morishima, S. (2021). <b>3D car shape reconstruction from a contour sketch using GAN and lazy learning.</b> The Visual Computer, 1-14.</p>
												<h4>国际会议</h4>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2022). <b>360 Depth Estimation in the Wild-The Depth360 Dataset and the SegFuse Network.</b> In 2022 IEEE conference on virtual reality and 3D user interfaces (VR). IEEE.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2021). <b>Bi-projection-based Foreground-aware Omnidirectional Depth Prediction.</b> Visual Computing Symposium 2021.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b> International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2020.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> Computer Graphics International 2020.</p>
												<p><b>Feng, Q.</b>, Shum, H. P., & Morishima, S. (2018, November). <b>Resolving occlusion for 3D object manipulation with hands in mixed reality.</b> In Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology.</p>
												<p><b>Feng, Q.</b>, Nozawa, T., Shum, H. P., & Morishima, S. (2018, August). <b>Occlusion for 3D Object Manipulation with Hands in Augmented Reality.</b> In Proceedings of The 21st Meeting on Image Recognition and Understanding.</p>
										</article>
									</div>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>研究主题</h3>
									<p>发布于Github上的开源项目: </p>
									<div class="features">
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/segfuse.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">360 Depth Estimation in the Wild</a></h4>
												<p>我们首先提出了一种从丰富的互联网360度全景视频中生成大量匹配的颜色/深度训练数据的方法。
													其次我们提出了一个多任务网络来学习360度图像的单眼深度预测。实验结果表面预测结果高效且准确。</p>
											</div>
										</article>
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/foreground.jpg" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">Foreground-aware Dense Depth Estimation for 360 Images</a></h4>
												<p>我们首先通过图像处理的方法来获取了不同样式的前景颜色/深度的训练数据。用新颖的方法将其合成至现有的360度图像的训练集后，
													我们提出了一个多任务辅助网络和损失函数来利用生成的数据，成功克服了现有方法对前景对象预测结果较差的问题。</p>
											</div>
										</article>
										<article>
											<a href="https://github.com/HAL-lucination/cyclegan-hand" class="image"><img src="images/hand.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://github.com/HAL-lucination/cyclegan-hand">Resolving Hand-Object Occlusion in Mixed Reality</a></h4>
												<p>为了解决混合现实(MR)中常见的由于场景结构未知而导致的手与物体互动时的遮蔽问题，我们首先通过CycleGAN生成了大规模
													真实且准确的颜色/深度/姿势训练集，然后提出了一个基于姿势和语义分割的多任务网络的实时系统。使用者实验和量化分析都获得了很好的结果。</p>
											</div>
										</article>
									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>联系方式</h3>
									<p>电话: +81-3-5286-3510<br />
										传真: +81-3-5286-3510<br />
										地址: <a href="https://goo.gl/maps/qCkFEBYzue7V8PMCA">55N406 3-4-1 Okubo, Shinjuku-ku, Tokyo, 169-0072</a><br />
										邮件: fengqi[at]ruri.waseda.jp</p>
									<form action="http://formspree.io/fengqi@ruri.waseda.jp" method="POST">
										<div class="row gtr-uniform">
											<div class="col-6 col-12-xsmall"><input type="text" name="name" id="name" placeholder="Name" /></div>
											<div class="col-6 col-12-xsmall"><input type="email" name="email" id="email" placeholder="Email" /></div>
											<div class="col-12"><input type="text" name="subject" id="subject" placeholder="Subject" /></div>
											<div class="col-12"><textarea name="message" id="message" placeholder="Message" rows="6"></textarea></div>
											<div class="col-12">
												<ul class="actions">
													<li><input type="submit" class="primary" value="Send Message" /></li>
													<li><input type="reset" value="Reset Form" /></li>
												</ul>
											</div>
										</div>
									</form>
								</div>
							</section>
					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Qi Feng All rights reserved.</li><li>Last update: 2022 Oct. 10</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
