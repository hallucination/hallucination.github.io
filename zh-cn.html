<!DOCTYPE HTML>
<html>
	<head>
		<title>Qi Feng | 冯 起</title>
		<link rel="icon" type="image/x-icon" href="/images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo"><a href="#">冯 起</a></h1>
					<p>图像处理和机器视觉 Ph.D. <br />
					助理教授 <br />
					早稻田大学, 森岛研究室</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">个人简介</a></li>
						<li><a href="#two">论文成果</a></li>
						<li><a href="#three">研究主题</a></li>
						<li><a href="#four">联系方式</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://github.com/HAL-lucination/" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/qi-feng-2680a6149/" class="icon brands fa-linkedin"><span class="label">Email</span></a></li>
						<li><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&authuser=1&user=17y69bAAAAAJ" class="icon solid fa-graduation-cap"><span class="label">Google Scholar</span></a></li>
						<li><a href="mailto: fengqi@ruri.waseda.jp" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						<br />
						<li><a href="/index.html">English</a></li>
						<li><a href="/ja.html">日本語</a></li>
						<li><a href="/fr.html">Français</a></li><br />
						<li><a href="/zh-cn.html">简体中文</a></li>
						<li><a href="/zh-cht.html">繁體中文</a></li>

					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="image main" data-position="center">
									<img src="images/banner.jpg" alt="" />
								</div>
								<div class="container">
									<header class="major">
										<h2>冯 起</h2>
										<h4><p> 我于2022年9月获得图像处理专业的博士学位。<br />
											现在在位于日本东京的早稻田大学 <br />
											先进理工学部担任助理教授(Assistant Professor)职位。<br />
											目前隶属于<a href="https://morishima-lab.jp/">森岛研究室</a>进行图像处理和机器视觉相关的研究工作。</p></h4>
									</header>
									<div class="features">
										<article>
												<h3>研究方向</h3>
												<h5><p> 我的研究方向主要是通过基于深度学习的方法解决不同的计算机图形和计算机视觉问题。
													除此之外，我也热衷于活用CG/CV的方法来解决一些当前虚拟现实(VR)和增强现实(AR)应用中存在的一系列实际问题。</p></h5>
										</article>
										<article>
												<h3>教育经历</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>2019年9月 - 2022年9月</h4>
													工程博士学位 - Ph.D. <br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2017年9月 - 2019年9月</h4>
													工程硕士学位 - M.E. <br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2013年9月 - 2017年9月</h4>
													工学学士学位 - B.E. <br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2010年9月 - 2013年9月</h4>
													高中毕业 <br />
													复旦大学附属中学 </li>
												</ul>
										</article>
										<article>
												<h3>工作经历</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>助理教授</h4>
													2021年4月 - 至今<br />
													早稻田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究交换</h4>
													2019年10月 - 2020年3月 <br />
													诺桑比亚大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究实习</h4>
													2019年7月 - 2019年9月 <br />
													日本产业技术综合研究所 (AIST) </li>
													<li class="icon solid fa-graduation-cap"><h4>研究实习</h4>
													2015年7月 - 2015年9月 <br />
													复旦大学 </li>
												</ul>
										</article>
										<article>
												<h3>技能一览</h3>
												<h4>语言</h4>
												<p><b>中文</b> - 母语水平 &nbsp;&nbsp;&nbsp;&nbsp; <b>英语</b> - 熟练掌握 (GRE得分325) <br />
												<b>日语</b> - 熟练掌握 (JLPT N1) &nbsp;&nbsp;&nbsp;&nbsp; <b>法语</b> - 简单对话 (CEFR A2) </p>
												<h4>编程语言</h4>
												<p>熟练使用: <b>Python</b>, HTML/CSS, SQL &nbsp;&nbsp;&nbsp;&nbsp; 较为熟悉: C++, C#, Java, Javascript</p>
												<h4>库/架构/平台</h4>
												<p>PyTorch, Torchvision, Tensorflow, OpenCV &nbsp;&nbsp;&nbsp;&nbsp; Git, SharePoint &nbsp;&nbsp;&nbsp;&nbsp;  WordPress, Unity3D, Arduino</p>
												<h4>深度学习相关技能</h4>
												<p>对象分类, 图像语义分割, 深度预测, 场景重建, 动作预测, 画风转换, 合成训练集</p>
												<h4>其他</h4>
												<p>Microsoft Office 365 (Access, SharePoint), Adobe Creative Cloud (Lightroom Classic, Photoshop, Illustrator, After Effects, Premiere Pro, Audition, InDesign), Ableton, Vocaloid, Blender</p><br />
										</article>
									</div>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<div class="features">
										<article>
											<h3>论文成果</h3>
											<h4>国际会议</h4>
											<p>Nishizawa, T., Tanaka, K., Hirata, A., Yamaguchi, S., <b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Hamanaka, M., & Morishima, S. (2025). <b>SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering.</b> In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision.</p>
											<p>Higasa, T., Tanaka, K., <b>Feng, Q.</b> <i class="fa fa-envelope"></i>, & Morishima, S. (2024). <b>Keep Eyes on the Sentence: An Interactive Sentence Simplification System for English Learners Based on Eye Tracking and Large Language Models.</b> CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, & Morishima, S. (2024). <b>Projection-Based Monocular Depth Prediction for 360 Images with Scale Awareness.</b> Visual Computing Symposium 2024.</p>
											<p>Inoue, R., <b>Feng, Q.</b>, & Morishima, S. (2024). <b>Non-Dominant Hand Skill Acquisition with Inverted Visual Feedback in a Mixed Reality Environment.</b> Visual Computing Symposium 2024.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2023). <b>Enhancing perception and immersion in pre-captured environments through learning-based eye height adaptation.</b> 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR).</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2023). <b>Learning Omnidirectional Depth Estimation from Internet Videos.</b> In Proceedings of the 26th Meeting on Image Recognition and Understanding.</p>
											<p>Higasa, T., Tanaka, K., <b>Feng, Q.</b>, & Morishima, S. (2023). <b>Gaze-Driven Sentence Simplification for Language Learners: Enhancing Comprehension and Readability.</b> The 25th International Conference on Multimodal Interaction (ICMI).</p>
											<p>Kashiwagi, S., Tanaka, K., <b>Feng, Q.</b>, & Morishima, S. (2023). <b>Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning.</b> INTERSPEECH 2023.</p>
											<p>Oshima, R., Shinagawa, S., Tsunashima, H., <b>Feng, Q.</b>, & Morishima, S. (2023). <b>Pointing out Human Answer Mistakes in a Goal-Oriented Visual Dialogue.</b> ICCV '23 Workshop and Challenge on Vision and Language Algorithmic Reasoning (ICCVW).</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2022). <b>360 Depth Estimation in the Wild-The Depth360 Dataset and the SegFuse Network.</b> In 2022 IEEE conference on virtual reality and 3D user interfaces (VR). IEEE.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2021). <b>Bi-projection-based Foreground-aware Omnidirectional Depth Prediction.</b> Visual Computing Symposium 2021.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b> International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2020.</p>
											<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> Computer Graphics International 2020.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2018, November). <b>Resolving occlusion for 3D object manipulation with hands in mixed reality.</b> In Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Nozawa, T., Shum, H. P., & Morishima, S. (2018, August). <b>Occlusion for 3D Object Manipulation with Hands in Augmented Reality.</b> In Proceedings of The 21st Meeting on Image Recognition and Understanding.</p>
											<h4>杂志期刊</h4>
											<p>Nozawa, N., Shum, H. P., <b>Feng, Q.</b>, Ho, E. S., & Morishima, S. (2021). <b>3D car shape reconstruction from a contour sketch using GAN and lazy learning.</b> The Visual Computer, 1-14.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2020). <b>Resolving hand‐object occlusion for mixed reality with joint deep learning and model optimization.</b>
												Computer Animation and Virtual Worlds, 31(4-5), e1956.</p>
											<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b>, Journal of WSCG, 28(1-2), 79-88.</p>
											<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> The Visual Computer, 36(10), 2117-2128.</p>
									</article>
									</div>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>研究主题</h3>
									<p>发布于Github上的开源项目: </p>
									<div class="features">
										<article>
											<a href="https://github.com/Kakanat/SyncViolinist" class="image"><img src="images/syncviolin.jpg" alt="" /></a>
											<div class="inner">
												<h4><a href="https://github.com/Kakanat/SyncViolinist">SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering</a></h4>
												<p>我们提出的SyncViolinist是一个多阶段端到端框架，可完全通过音频输入生成同步的小提琴演奏动作。它成功克服了同时捕捉全局和细微演奏特征的难题。</p>
											</div>
										</article>
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/segfuse.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">360 Depth Estimation in the Wild</a></h4>
												<p>我们首先提出了一种从丰富的互联网360度全景视频中生成大量匹配的颜色/深度训练数据的方法。
													其次我们提出了一个多任务网络来学习360度图像的单眼深度预测。实验结果高效且准确。</p>
											</div>
										</article>
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/foreground.jpg" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">Foreground-aware Dense Depth Estimation for 360 Images</a></h4>
												<p>我们首先通过图像处理的方法来获取了不同样式的前景颜色/深度的训练数据。用新颖的方法将其合成至现有的360度图像的训练集后，
													我们继而提出了一个多任务辅助网络和损失函数，成功克服了现有方法对前景对象预测结果较差的问题。</p>
											</div>
										</article>
										<article>
											<a href="https://github.com/HAL-lucination/cyclegan-hand" class="image"><img src="images/hand.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://github.com/HAL-lucination/cyclegan-hand">Resolving Hand-Object Occlusion in Mixed Reality</a></h4>
												<p>为了解决混合现实(MR)中常见的由于场景结构未知而导致的手与物体间的遮蔽问题，我们通过CycleGAN生成了大规模
													真实且准确的颜色/深度/姿势训练集，然后提出了一个实时的基于姿势追踪和语义分割的多任务网络。使用者实验和量化分析都获得了很好的结果。</p>
											</div>
										</article>
									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>联系方式</h3>
									<p>电话: +81-3-5286-3510<br />
										传真: +81-3-5286-3510<br />
										地址: <a href="https://goo.gl/maps/qCkFEBYzue7V8PMCA">55N406 3-4-1 Okubo, Shinjuku-ku, Tokyo, 169-0072</a><br />
										邮件: fengqi[at]ruri.waseda.jp</p>
									<form action="http://formspree.io/fengqi@ruri.waseda.jp" method="POST">
										<div class="row gtr-uniform">
											<div class="col-6 col-12-xsmall"><input type="text" name="name" id="name" placeholder="Name" /></div>
											<div class="col-6 col-12-xsmall"><input type="email" name="email" id="email" placeholder="Email" /></div>
											<div class="col-12"><input type="text" name="subject" id="subject" placeholder="Subject" /></div>
											<div class="col-12"><textarea name="message" id="message" placeholder="Message" rows="6"></textarea></div>
											<div class="col-12">
												<ul class="actions">
													<li><input type="submit" class="primary" value="Send Message" /></li>
													<li><input type="reset" value="Reset Form" /></li>
												</ul>
											</div>
										</div>
									</form>
								</div>
							</section>
					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Qi Feng All rights reserved.</li><li>Last update: 2024 Oct. 31</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
