<!DOCTYPE HTML>
<html>
	<head>
		<title>Qi Feng | 馮 起</title>
		<link rel="icon" type="image/x-icon" href="/images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo"><a href="#">馮　起</a></h1>
					<p>画像処理 博士<br />
					助教授 (Assistant Professor) <br />
					早稲田大学 森島研究室</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">自己紹介</a></li>
						<li><a href="#two">研究成果</a></li>
						<li><a href="#three">プロジェクト</a></li>
						<li><a href="#four">連絡先</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="https://github.com/HAL-lucination/" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/qi-feng-2680a6149/" class="icon brands fa-linkedin"><span class="label">Email</span></a></li>
						<li><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&authuser=1&user=17y69bAAAAAJ" class="icon solid fa-graduation-cap"><span class="label">Google Scholar</span></a></li>
						<li><a href="mailto: fengqi@ruri.waseda.jp" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						<br />
						<li><a href="/index.html">English</a></li>
						<li><a href="/ja.html">日本語</a></li>
						<li><a href="/fr.html">Français</a></li><br />
						<li><a href="/zh-cn.html">简体中文</a></li>
						<li><a href="/zh-cht.html">繁體中文</a></li>

					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="image main" data-position="center">
									<img src="images/banner.jpg" alt="" />
								</div>
								<div class="container">
									<header class="major">
										<h4>ヒョウ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;キ</h4>
										<h2>馮 起</h2>
										<h4><p> 2022年9月で博士号取得。<br />
											現在、早稲田大学先進理工学部の物理・応用物理学科で <br />
											助教授(Assistant Professor)として働いています。<br />
											<a href="https://morishima-lab.jp/">森島研究室</a>に所属、コンピューターグラフィックスとコンピュータービションに関する研究を行っています。</p></h4>
									</header>
									<div class="features">
										<article>
												<h3>研究テーマ</h3>
												<h5><p> 深層学習とデータ生成の手法を用いて、コンピュータグラフィックス(CG)やコンピュータビジョン(CV)に関する研究を行っています。
													また、仮想現実(VR)や拡張現実(AR)における重要かつ難しい課題にCG/CVの手法を活用して取り込んでいます。</p></h5>
										</article>
										<article>
												<h3>学歴</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>2019年9月 - 2022年9月</h4>
													工学博士 - Ph.D. <br />
													早稲田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2017年9月 - 2019年9月</h4>
													工学修士 - M.E. <br />
													早稲田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2013年9月 - 2017年9月</h4>
													工学学士 - B.E. <br />
													早稲田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>2010年9月 - 2013年9月</h4>
													高校卒業 <br />
													復旦大学付属中学 </li>
												</ul>
										</article>
										<article>
												<h3>履歴</h3>
												<ul class="feature-icons">
													<li class="icon solid fa-graduation-cap"><h4>助教授</h4>
													2021年4月 - 現在<br />
													早稲田大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究インターン</h4>
													2019年10月 - 2020年3月 <br />
													ノーザンブリア大学 </li>
													<li class="icon solid fa-graduation-cap"><h4>研究インターン</h4>
													2019年7月 - 2019年9月 <br />
													産業技術総合研究所 (AIST) </li>
													<li class="icon solid fa-graduation-cap"><h4>研究インターン</h4>
													2015年7月 - 2015年9月 <br />
													中国復旦大学 </li>
												</ul>
										</article>
										<article>
												<h3>スキル</h3>
												<h4>言語</h4>
												<p><b>中国語</b> - ネイティブレベル &nbsp;&nbsp;&nbsp;&nbsp; <b>英語</b> - ほぼネイティブレベル (GREスコア325, TOEICスコア985) <br />
												<b>日本語</b> - ビジネスレベル (JLPT N1) &nbsp;&nbsp;&nbsp;&nbsp; <b>フランス語</b> - 日常会話レベル (CEFR A2) </p>
												<h4>プログラミング言語</h4>
												<p>熟練: <b>Python</b>, HTML/CSS, SQL &nbsp;&nbsp;&nbsp;&nbsp; その他: C++, C#, Java, Javascript</p>
												<h4>ライブラリ/フレームワークス/プラットフォーム</h4>
												<p>PyTorch, Torchvision, Tensorflow, OpenCV &nbsp;&nbsp;&nbsp;&nbsp; Git, SharePoint &nbsp;&nbsp;&nbsp;&nbsp;  WordPress, Unity3D, Arduino</p>
												<h4>深層学習</h4>
												<p>物体分類、領域分類、深度予測、シーン再構成、姿勢推定、画風変換、データ合成</p>
												<h4>その他</h4>
												<p>Microsoft Office 365 (Access, SharePoint), Adobe Creative Cloud (Lightroom Classic, Photoshop, Illustrator, After Effects, Premiere Pro, Audition, InDesign), Ableton, Vocaloid, Blender</p><br />
										</article>
									</div>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<div class="features">
										<article>
												<h3>研究成果</h3>
												<h4>国際学会</h4>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Kayukawa, S., Wang, X., Takagi, H., & Asagawa, C. (Under Review). <b>STAMP: Scale-aware TActile-Map Printing for Accessible Spatial Understanding and Navigation.</b> In 2026 CHI Conference on Human Factors in Computing Systems. </p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Kayukawa, S., Wang, X., Takagi, H., & Asagawa, C. (2025). <b>A Scale-Aware Method for Generating 3D-Printable Tactile Maps.</b> The 33rd Workshop on Interactive Systems and Software. </p>
												<p>Iwakata, S., Oshima, R., Tsunashima, H., <b>Feng, Q.</b>, Kataoka, H., & Morishima, S. (2025). <b>Viewpoint-dependent 3D Visual Grounding for Mobile Robots. </b>In 2025 IEEE International Conference on Image Processing.</p>
												<p>Inoue, R., <b>Feng, Q.</b> <i class="fa fa-envelope"></i>, & Morishima, S. (2025). <b>SynchroDexterity: Rapid Non-Dominant Hand Skill Acquisition with Synchronized Guidance in Mixed Reality.</b> In 2025 IEEE conference on virtual reality and 3D user interfaces (VR). IEEE.</p>
												<p>Nishizawa, T., Tanaka, K., Hirata, A., Yamaguchi, S., <b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Hamanaka, M., & Morishima, S. (2025). <b>SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering.</b> In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision.</p>
												<p>Higasa, T., Tanaka, K., <b>Feng, Q.</b> <i class="fa fa-envelope"></i>, & Morishima, S. (2024). <b>Keep Eyes on the Sentence: An Interactive Sentence Simplification System for English Learners Based on Eye Tracking and Large Language Models.</b> CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, & Morishima, S. (2024). <b>Projection-Based Monocular Depth Prediction for 360 Images with Scale Awareness.</b> Visual Computing Symposium 2024.</p>
												<p>Inoue, R., <b>Feng, Q.</b>, & Morishima, S. (2024). <b>Non-Dominant Hand Skill Acquisition with Inverted Visual Feedback in a Mixed Reality Environment.</b> Visual Computing Symposium 2024.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2023). <b>Enhancing perception and immersion in pre-captured environments through learning-based eye height adaptation.</b> 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR).</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2023). <b>Learning Omnidirectional Depth Estimation from Internet Videos.</b> In Proceedings of the 26th Meeting on Image Recognition and Understanding.</p>
												<p>Higasa, T., Tanaka, K., <b>Feng, Q.</b>, & Morishima, S. (2023). <b>Gaze-Driven Sentence Simplification for Language Learners: Enhancing Comprehension and Readability.</b> The 25th International Conference on Multimodal Interaction (ICMI).</p>
												<p>Kashiwagi, S., Tanaka, K., <b>Feng, Q.</b>, & Morishima, S. (2023). <b>Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning.</b> INTERSPEECH 2023.</p>
												<p>Oshima, R., Shinagawa, S., Tsunashima, H., <b>Feng, Q.</b>, & Morishima, S. (2023). <b>Pointing out Human Answer Mistakes in a Goal-Oriented Visual Dialogue.</b> ICCV '23 Workshop and Challenge on Vision and Language Algorithmic Reasoning (ICCVW).</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2022). <b>360 Depth Estimation in the Wild-The Depth360 Dataset and the SegFuse Network.</b> In 2022 IEEE conference on virtual reality and 3D user interfaces (VR). IEEE.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2021). <b>Bi-projection-based Foreground-aware Omnidirectional Depth Prediction.</b> Visual Computing Symposium 2021.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b> International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2020.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> Computer Graphics International 2020.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2018, November). <b>Resolving occlusion for 3D object manipulation with hands in mixed reality.</b> In Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Nozawa, T., Shum, H. P., & Morishima, S. (2018, August). <b>Occlusion for 3D Object Manipulation with Hands in Augmented Reality.</b> In Proceedings of The 21st Meeting on Image Recognition and Understanding.</p>
												<h4>ジャーナル</h4>
												<p>Nozawa, N., Shum, H. P., <b>Feng, Q.</b>, Ho, E. S., & Morishima, S. (2021). <b>3D car shape reconstruction from a contour sketch using GAN and lazy learning.</b> The Visual Computer, 1-14.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., & Morishima, S. (2020). <b>Resolving hand‐object occlusion for mixed reality with joint deep learning and model optimization.</b>
													Computer Animation and Virtual Worlds, 31(4-5), e1956.</p>
												<p><b>Feng, Q.</b> <i class="fa fa-envelope"></i>, Shum, H. P., Shimamura, R., & Morishima, S. (2020). <b>Foreground-aware Dense Depth Estimation for 360 Images.</b>, Journal of WSCG, 28(1-2), 79-88.</p>
												<p>Shimamura, R., <b>Feng, Q.</b>, Koyama, Y., Nakatsuka, T., Fukayama, S., Hamasaki, M., ... & Morishima, S. (2020). <b>Audio–visual object removal in 360-degree videos.</b> The Visual Computer, 36(10), 2117-2128.</p>
										</article>
									</div>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>プロジェクト</h3>
									<p>Githubで公開されているプロジェクト： </p>
									<div class="features">
										<article>
											<a href="https://github.com/Kakanat/SyncViolinist" class="image"><img src="images/syncviolin.jpg" alt="" /></a>
											<div class="inner">
												<h4><a href="https://github.com/Kakanat/SyncViolinist">SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering</a></h4>
												<p>SyncViolinistは，音声入力のみを用いて同期したバイオリン演奏動作を生成する多段階のエンドツーエンドフレームワークである．本フレームワークは，演奏のグローバルな特徴と細部の特徴を効果的に捉える課題を克服することに成功している．</p>
											</div>
										</article>
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/segfuse.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">360 Depth Estimation in the Wild</a></h4>
												<p>ネット上の豊富な360度パノラマ動画を活用し、大規模なRGB・深度の学習データを生成する方法を提案した。
													さらに、360度画像に対する単眼的深度予測を学習するマルチタスクモデルを提案した。実験の結果、予測は効率的で正確であることが確認した。</p>
											</div>
										</article>
										<article>
											<a href="https://segfuse.github.io/" class="image"><img src="images/foreground.jpg" alt="" /></a>
											<div class="inner">
												<h4><a href="https://segfuse.github.io/">Foreground-aware Dense Depth Estimation for 360 Images</a></h4>
												<p>360度のRGB・深度データセットに現実的な前景を追加し、大規模な全景を意識する学習データを生成した。
                          また、前景の深度・セグメンテーションを推定する補助ネットワークを提案し、より精度の高い推定を実現した。</p>
											</div>
										</article>
										<article>
											<a href="https://github.com/HAL-lucination/cyclegan-hand" class="image"><img src="images/hand.gif" alt="" /></a>
											<div class="inner">
												<h4><a href="https://github.com/HAL-lucination/cyclegan-hand">Resolving Hand-Object Occlusion in Mixed Reality</a></h4>
												<p>強いオクルージョン下での姿勢追跡とセグメンテーションのために、CycleGANを用いたRGBD手と物体のデータセットを提案した。
                            そして、最適化を用いて拡張現実におけるオクルージョン問題を実時間で解決した。</p>
											</div>
										</article>
									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>連絡先</h3>
									<p>電話番号: +81-3-5286-3510<br />
										FAX: +81-3-5286-3510<br />
										住所: <a href="https://goo.gl/maps/qCkFEBYzue7V8PMCA">55N406 3-4-1 Okubo, Shinjuku-ku, Tokyo, 169-0072</a><br />
										メール: fengqi[at]ruri.waseda.jp</p>
									<form action="http://formspree.io/fengqi@ruri.waseda.jp" method="POST">
										<div class="row gtr-uniform">
											<div class="col-6 col-12-xsmall"><input type="text" name="name" id="name" placeholder="Name" /></div>
											<div class="col-6 col-12-xsmall"><input type="email" name="email" id="email" placeholder="Email" /></div>
											<div class="col-12"><input type="text" name="subject" id="subject" placeholder="Subject" /></div>
											<div class="col-12"><textarea name="message" id="message" placeholder="Message" rows="6"></textarea></div>
											<div class="col-12">
												<ul class="actions">
													<li><input type="submit" class="primary" value="Send Message" /></li>
													<li><input type="reset" value="Reset Form" /></li>
												</ul>
											</div>
										</div>
									</form>
								</div>
							</section>
					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Qi Feng All rights reserved.</li><li>Last update: 2024 Oct. 31</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
